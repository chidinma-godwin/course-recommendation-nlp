# Project Title
Conversational Online Course Recommendation System Using Retrieval Augmented Generation

## Description
This repository contains code implementations for a Conversational Online Course Recommendation System. The project leverages machine learning and natural language processing techniques to recommend courses based on user queries. The project includes scraping scripts, code for data generation, model training and a recommendation pipeline.

## Data Sources
The course dataset was scrapped from [https://www.classcentral.com](https://www.classcentral.com) using Scrapy library. Step-by-step intruction on how the scrapping was done can be found in **scrapping_script.ipynb** and the scrapping setup files are in the **coursecrapper** folder. The setup files are automatically generated by Scrapy but are modified to configure the settings for scraping the information needed for this project.

The query dataset is a combination of the **CLINC150** dataset loaded from **hugging face** and manually generated data augmented by paraphrasing. Documentation on the  CLINC150 dataset can be found on [hugging face](https://huggingface.co/datasets/clinc/clinc_oos)

## Project Structure
```plaintext
├── coursescrapper/                 # Directory containing Scrapy settings files 
├── course_recommendation.ipynb     # Main notebook for the course recommendation pipeline
├── courseslist.csv                 # File containing the course data
├── generate_query_data.ipynb       # Notebook for generating the query data
├── queries_df.csv                  # File containing the query data
└── scrapping_script.ipynb          # Notebook for scraping course data from class central website
```

## Getting Started
All notebook files were created and executed on Google Colab, except for **scraping_script.ipynb**, which was developed and run locally due to the time required for data scraping.

To avoid getting error when loading the course and query data, place the courseslist.csv and queries_df.csv files in a project folder on google drive, or change the directory in ```pd.read_csv``` to point to where the files are located. Ensure that all necessary libraries are installed.
### Dependencies Not Pre-installed in Google Colab
- sentence_transformers: For creating sentence embeddings
- adjustText: For automatically adjusting the positioning of text labels in plots to prevent overlaps (Used in the visualization of the embeddings)
- keras-tuner: used for hyperparameter tuning of the LSTM-based model
- datasets: A Hugging Face library that provides easy access to a wide variety of datasets (Used to load the CLINC150 dataset for the intent detection and slot filling task)
- spacy: Fast NLP library for text processing

## Key Functions Used in This Project
- create_combined_details:
  - description: Combines details from multiple columns into a single descriptive summary
  - parameter: A pandas dataframe row
- preprocess_texts:
  - description: Preprocesses a list of texts by converting to lowercase, removing punctuation, and lemmatizing the words while preserving periods between numbers.
  - parameter: A list of text strings.
- recommend_courses_cosine:
  - description: Recommends courses based on cosine similarity between a user query embedding and course embeddings.
  - parameters:
      - user_query_embedding: The embedding of the user's query.
      - course_embeddings: Embeddings for the available courses.
      - data: DataFrame containing course information.
      - overwrite_score (optional): If True, updates the cosine similarity scores in the data.
      - top_n (optional): Number of top courses to return.
      - cut_off (optional): Minimum similarity score to filter courses.
- predict_intent_and_slots:
  - description: Predicts the intent and slots from a given sentence using the trained LSTM-based model.
  - parameters:
      - sentence: The input sentence to be analyzed.
      - label_encoder: Encoder for decoding intent labels.
      - slot_encoder: Encoder for decoding slot labels.
- filter_by_rating:
  - description: Filters the course dataframe based on a rating value that can be a number or a comparison keyword.
  - parameters:
    - df: A DataFrame containing a column avg_rating.
    - rating_value: A string specifying the rating threshold or comparison (e.g., "4", "above 4.5").
- get_matching_course:
  - description: Filters courses from a DataFrame based on specified slot values using either TF-IDF or Word2Vec for similarity scoring.
  - parameters:
    - slots: A dictionary with slot names as keys and slot values as values.
    - course_df: The courses dataframe.
    - top_n: The number of top matching courses to return (default is 3).
    - method: The similarity method to use ("tfidf" or "word2vec").
- get_messages:
  - description: Creates a formatted prompt for the LLM based on user query, context (retrieved courses), intent, and chat history
  - parameters:
    - query: The user's current question or input text.
    - context_df: DataFrame containing contextual information about courses.
    - intent: The intent behind the user's query (recommend, details, oos, courtesy).
    - chat_history: List of previous user queries and the model responses.
- ask:
  - description: Process a user query to predict intent and slots, find matching courses, and generate a response based on the retrieved course.
  - parameters:
    - query: The user's query to process.
    - temperature (float): Sampling temperature for text generation (default 0.7).
    - max_new_tokens (int): Maximum number of tokens to generate (default 512).
    - print_info (bool): Whether to print the user query, extracted intent and slots (default False).

## Optimized Hyperparameters for the Joint Intent Classification and Slot Filling Model
- Embedding Dimension: 200
- Number of LSTM Layers: 1
- LSTM Units: 512
- Bidirectional LSTM Units: 32
- Dense Units for the Slots Output: 64
- Activation Function for Slots Output Dense Layer: tanh
- Intent Dropout Rate: 0.6
- Slot Dropout Rate: 0.8
- Optimizer: rmsprop
- Batch Size: 16

## Acknowledgments
This project uses pre-trained models (LLM, sentence transformer, cross encoder, paraphrasing models) from Hugging Face and datasets from the CLINC150 dataset. Special thanks to the creators of these resources for their contributions to the machine learning community.